{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79acc616-0141-4de7-be29-5ba59e9ef4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "# ==============================================================================\n",
    "# --- Configuration Section (Modify all tunable parameters here) ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. File Path Configuration ---\n",
    "INPUT_FILE = 'Original dataset-BQ.xlsx'  # Input Excel filename\n",
    "OUTPUT_DIR = 'feature_engineering_BQ_output' # Directory name to save all output results\n",
    "\n",
    "# --- 2. Core Algorithm Parameters ---\n",
    "RANDOM_STATE = 0  # Random seed to ensure reproducibility. Usually does not need to be changed.\n",
    "PEARSON_CORR_THRESHOLD = 0.8  # Pearson correlation coefficient threshold for Stage 1. Feature pairs with a correlation greater than this value will be processed.\n",
    "\n",
    "# --- 3. Random Forest Model Hyperparameters ---\n",
    "RF_N_ESTIMATORS = 300  # Number of trees in the forest. A higher value makes the model more stable but increases computation time.\n",
    "RF_MIN_SAMPLES_LEAF = 5  # The minimum number of samples required to be at a leaf node.\n",
    "RF_MIN_SAMPLES_SPLIT = 10  # The minimum number of samples required to split an internal node.\n",
    "RF_MAX_FEATURES = 'sqrt'  # The number of features to consider when looking for the best split. 'sqrt' uses the square root of the total number of features.\n",
    "RF_MAX_DEPTH = None  # The maximum depth of the tree. None means nodes are expanded until all leaves are pure.\n",
    "RF_N_JOBS = -1  # The number of jobs to run in parallel. -1 means using all available CPU cores to speed up computation.\n",
    "\n",
    "# --- 4. SHAP Value and Iterative Selection Parameters ---\n",
    "PERFORMANCE_METRIC = 'r2'  # Performance metric for iterative selection. Options: 'mae' (Mean Absolute Error, lower is better) or 'r2' (R-squared, higher is better).\n",
    "SHAP_COARSE_SELECTION_PERCENT = 0.8  # Percentage of features to keep during the SHAP coarse selection stage. 1.0 means keep 100%, 0.8 means keep the top 80%.\n",
    "EARLY_STOPPING_PATIENCE = 5 # Early stopping patience. The number of consecutive iterations with no performance improvement before stopping.\n",
    "\n",
    "# --- 5. Cross-Validation Parameters ---\n",
    "KFOLD_SPLITS = 10 # Number of folds (k) for K-Fold cross-validation. Common values are 5 or 10.\n",
    "\n",
    "# --- 6. Dataset Splitting Parameters ---\n",
    "# Slice rule for feature columns (string format). '1:-1' means from the second column to the second-to-last.\n",
    "FEATURE_COLUMN_SLICE = '1:-1'\n",
    "# Index for the target column (integer format). -1 means the last column.\n",
    "TARGET_COLUMN_INDEX = -1\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Core Logic (Usually no modification is needed below this line) ---\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import shap\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import textwrap\n",
    "\n",
    "def load_data_from_excel():\n",
    "    \"\"\"\n",
    "    Loads data from the specified Excel file, extracts features (X) and the target (Y)\n",
    "    based on the global configuration, and performs data cleaning.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from '{INPUT_FILE}'...\")\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"Error: File '{INPUT_FILE}' not found.\")\n",
    "        exit()\n",
    "\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    df_original = df.copy()\n",
    "\n",
    "    try:\n",
    "        all_col_names = df.columns\n",
    "        \n",
    "        try:\n",
    "            slice_parts = FEATURE_COLUMN_SLICE.split(':')\n",
    "            start = int(slice_parts[0]) if slice_parts[0] else None\n",
    "            end = int(slice_parts[1]) if len(slice_parts) > 1 and slice_parts[1] else None\n",
    "            feature_slice = slice(start, end)\n",
    "            \n",
    "            feature_col_names = all_col_names[feature_slice]\n",
    "            target_col_name = all_col_names[TARGET_COLUMN_INDEX]\n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f\"Error: Invalid dataset split parameters '{FEATURE_COLUMN_SLICE}' or '{TARGET_COLUMN_INDEX}'. Please check the configuration. Details: {e}\")\n",
    "            exit()\n",
    "\n",
    "        if len(feature_col_names) > 1:\n",
    "            feature_range_str = f\"from column '{feature_col_names[0]}' to '{feature_col_names[-1]}'\"\n",
    "        elif len(feature_col_names) == 1:\n",
    "            feature_range_str = f\"column '{feature_col_names[0]}'\"\n",
    "        else:\n",
    "            feature_range_str = \"no features\"\n",
    "\n",
    "        print(f\"Extracting features ({feature_range_str}) and target '{target_col_name}'.\")\n",
    "\n",
    "        if target_col_name in feature_col_names:\n",
    "            print(f\"Critical Error: The target column '{target_col_name}' is also identified as a feature column. Please check the column order in your Excel file or the splitting parameters.\")\n",
    "            exit()\n",
    "\n",
    "        X = df[feature_col_names]\n",
    "        Y = df[target_col_name]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unknown error occurred while extracting columns: {e}.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Extracted features X with shape: {X.shape}\")\n",
    "    print(f\"Extracted target Y '{Y.name}' with shape: {Y.shape}\")\n",
    "\n",
    "    combined_df = pd.concat([X, Y], axis=1)\n",
    "    for col in combined_df.columns:\n",
    "        combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
    "\n",
    "    original_rows = len(combined_df)\n",
    "    combined_df_cleaned = combined_df.dropna()\n",
    "\n",
    "    if len(combined_df_cleaned) < original_rows:\n",
    "        print(f\"Warning: Dropped {original_rows - len(combined_df_cleaned)} rows containing missing values.\")\n",
    "        df_original = df_original.loc[combined_df_cleaned.index]\n",
    "\n",
    "    X_cleaned = combined_df_cleaned.drop(columns=[Y.name])\n",
    "    y_cleaned = combined_df_cleaned[Y.name]\n",
    "\n",
    "    print(f\"Data loading complete. Final shapes: X={X_cleaned.shape}, y={y_cleaned.shape}\")\n",
    "    return X_cleaned, y_cleaned, df_original\n",
    "\n",
    "def step1_filter_high_correlated_features(X, y):\n",
    "    \"\"\"\n",
    "    Stage 1: Filter out highly correlated features using the Pearson correlation coefficient.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Stage 1: Filtering Highly Correlated Features ---\")\n",
    "\n",
    "    if X.shape[1] <= 1:\n",
    "        print(\"Insufficient number of features (<=1). Skipping correlation filtering.\")\n",
    "        return X.copy(), []\n",
    "    if X.shape[0] <= 1:\n",
    "        print(\"Insufficient number of samples (<=1). Skipping correlation filtering.\")\n",
    "        return X.copy(), []\n",
    "\n",
    "    corr_matrix = X.corr(method='pearson').abs()\n",
    "    feature_y_corr = {col: abs(X[col].corr(y)) for col in X.columns}\n",
    "    sorted_features = sorted(X.columns, key=lambda col: feature_y_corr.get(col, -1), reverse=True)\n",
    "\n",
    "    kept_features_final = []\n",
    "    all_dropped_features_set = set()\n",
    "    retained_to_dropped_map = defaultdict(list)\n",
    "\n",
    "    for current_feature in sorted_features:\n",
    "        if current_feature in all_dropped_features_set:\n",
    "            continue\n",
    "        kept_features_final.append(current_feature)\n",
    "        for other_feature in sorted_features:\n",
    "            if other_feature == current_feature or other_feature in all_dropped_features_set:\n",
    "                continue\n",
    "            if corr_matrix.loc[current_feature, other_feature] > PEARSON_CORR_THRESHOLD:\n",
    "                all_dropped_features_set.add(other_feature)\n",
    "                retained_to_dropped_map[current_feature].append(other_feature)\n",
    "\n",
    "    to_drop_list = list(all_dropped_features_set)\n",
    "\n",
    "    if to_drop_list:\n",
    "        print(f\"Based on Pearson correlation (threshold > {PEARSON_CORR_THRESHOLD}), the following features were removed:\")\n",
    "        output_data = []\n",
    "        for kept_feat, dropped_list in retained_to_dropped_map.items():\n",
    "            if dropped_list:\n",
    "                output_data.append({\n",
    "                    'retained_feature': kept_feat,\n",
    "                    'dropped_features': \", \".join(sorted(dropped_list))\n",
    "                })\n",
    "        if output_data:\n",
    "            max_kept_len = max(len(row['retained_feature']) for row in output_data)\n",
    "            col1_header = 'Retained Feature'\n",
    "            col1_width = max(max_kept_len, len(col1_header)) + 4\n",
    "            col2_header = 'Dropped Features (due to high correlation)'\n",
    "            terminal_width = 120\n",
    "            col2_width = terminal_width - col1_width\n",
    "            print(f\"\\n{col1_header:<{col1_width}}{col2_header}\")\n",
    "            print(f\"{'-' * (col1_width - 1)} {'-' * len(col2_header)}\")\n",
    "            for row in output_data:\n",
    "                kept_feat = row['retained_feature']\n",
    "                dropped_feats_str = row['dropped_features']\n",
    "                wrapped_lines = textwrap.wrap(dropped_feats_str, width=col2_width)\n",
    "                print(f\"{kept_feat:<{col1_width}}{wrapped_lines[0] if wrapped_lines else ''}\")\n",
    "                for i in range(1, len(wrapped_lines)):\n",
    "                    print(f\"{'':<{col1_width}}{wrapped_lines[i]}\")\n",
    "        X_filtered = X.drop(columns=to_drop_list)\n",
    "    else:\n",
    "        print(\"No feature pairs exceeded the Pearson correlation threshold. No features were removed.\")\n",
    "        X_filtered = X.copy()\n",
    "\n",
    "    print(f\"\\nNumber of features after filtering: {X_filtered.shape[1]}\")\n",
    "    return X_filtered, to_drop_list\n",
    "\n",
    "\n",
    "def step2_embed_shap_coarse_selection(X_filtered, y):\n",
    "    \"\"\"\n",
    "    Stage 2: Coarse feature selection using an embedded method based on SHAP values.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Stage 2: SHAP Coarse Feature Selection ({KFOLD_SPLITS}-Fold CV) ---\")\n",
    "\n",
    "    if X_filtered.shape[1] == 0:\n",
    "        print(\"No features available for SHAP coarse selection.\")\n",
    "        return X_filtered.copy(), []\n",
    "    if X_filtered.shape[0] < KFOLD_SPLITS:\n",
    "        print(f\"Number of samples ({X_filtered.shape[0]}) is less than k_folds ({KFOLD_SPLITS}). Cannot perform cross-validation.\")\n",
    "        return X_filtered.copy(), []\n",
    "\n",
    "    kf = KFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    avg_abs_shap_values_per_fold = []\n",
    "\n",
    "    print(f\"Calculating SHAP values (using {KFOLD_SPLITS}-Fold Cross-Validation)...\")\n",
    "\n",
    "    train_size_per_fold = X_filtered.shape[0] * (KFOLD_SPLITS - 1) // KFOLD_SPLITS\n",
    "    min_samples_for_rf = max(RF_MIN_SAMPLES_LEAF, RF_MIN_SAMPLES_SPLIT)\n",
    "    if train_size_per_fold < min_samples_for_rf:\n",
    "        print(f\"Warning: Training samples per fold ({train_size_per_fold}) is less than the minimum required by Random Forest ({min_samples_for_rf}). Skipping SHAP selection.\")\n",
    "        return X_filtered.copy(), []\n",
    "\n",
    "    for train_idx, val_idx in tqdm(kf.split(X_filtered), total=KFOLD_SPLITS, desc=\"SHAP Coarse Selection\"):\n",
    "        X_fold_train, y_fold_train = X_filtered.iloc[train_idx], y.iloc[train_idx]\n",
    "        scaler = StandardScaler().fit(X_fold_train)\n",
    "        X_fold_train_scaled = scaler.transform(X_fold_train)\n",
    "        rf_model = RandomForestRegressor(n_estimators=RF_N_ESTIMATORS, max_features=RF_MAX_FEATURES, min_samples_leaf=RF_MIN_SAMPLES_LEAF, min_samples_split=RF_MIN_SAMPLES_SPLIT, random_state=RANDOM_STATE, n_jobs=RF_N_JOBS)\n",
    "        try:\n",
    "            rf_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "            explainer = shap.TreeExplainer(rf_model)\n",
    "            shap_values_fold = explainer.shap_values(X_fold_train_scaled)\n",
    "            if shap_values_fold.ndim == 1:\n",
    "                avg_abs_shap_values_per_fold.append(np.abs(shap_values_fold))\n",
    "            else:\n",
    "                avg_abs_shap_values_per_fold.append(np.abs(shap_values_fold).mean(axis=0))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Model training or SHAP calculation failed during coarse selection: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not avg_abs_shap_values_per_fold:\n",
    "        print(\"Warning: Failed to calculate any SHAP values. Skipping SHAP coarse selection.\")\n",
    "        return X_filtered.copy(), []\n",
    "\n",
    "    feature_importances_shap = pd.Series(np.mean(avg_abs_shap_values_per_fold, axis=0), index=X_filtered.columns).sort_values(ascending=False)\n",
    "    print(\"\\nFeature Importances (based on mean absolute SHAP values from CV):\")\n",
    "    print(feature_importances_shap)\n",
    "\n",
    "    num_features_to_keep = int(len(feature_importances_shap) * SHAP_COARSE_SELECTION_PERCENT)\n",
    "    if num_features_to_keep == 0 and len(feature_importances_shap) > 0: num_features_to_keep = 1\n",
    "    if num_features_to_keep > len(feature_importances_shap): num_features_to_keep = len(feature_importances_shap)\n",
    "\n",
    "    features_to_keep = feature_importances_shap.index[:num_features_to_keep].tolist()\n",
    "    features_to_drop = list(set(X_filtered.columns) - set(features_to_keep))\n",
    "\n",
    "    if features_to_drop:\n",
    "        print(f\"\\nBased on SHAP coarse selection (keeping top {SHAP_COARSE_SELECTION_PERCENT * 100:.1f}%), the following features were removed: {features_to_drop}\")\n",
    "        X_shap_coarse = X_filtered[features_to_keep]\n",
    "    else:\n",
    "        print(\"\\nNo features were removed after SHAP coarse selection.\")\n",
    "        X_shap_coarse = X_filtered.copy()\n",
    "\n",
    "    print(f\"Number of features after SHAP coarse selection: {X_shap_coarse.shape[1]}\")\n",
    "    return X_shap_coarse, features_to_drop\n",
    "\n",
    "\n",
    "def step3_wrapper_shap_iterative_selection(X_shap_coarse, y):\n",
    "    \"\"\"\n",
    "    Stage 3: Iterative fine-grained feature selection using a wrapper method with an early stopping mechanism.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Stage 3: Iterative Feature Fine-Tuning ({KFOLD_SPLITS}-Fold CV) ---\")\n",
    "\n",
    "    if X_shap_coarse.shape[1] <= 1:\n",
    "        print(\"Insufficient number of features (<=1). Skipping iterative selection.\")\n",
    "        return list(X_shap_coarse.columns), []\n",
    "    if X_shap_coarse.shape[0] < KFOLD_SPLITS:\n",
    "        print(f\"Number of samples ({X_shap_coarse.shape[0]}) is less than k_folds ({KFOLD_SPLITS}). Cannot perform cross-validation.\")\n",
    "        return list(X_shap_coarse.columns), []\n",
    "\n",
    "    current_features = list(X_shap_coarse.columns)\n",
    "    best_features = list(current_features)\n",
    "\n",
    "    if PERFORMANCE_METRIC == 'r2':\n",
    "        best_score = -np.inf\n",
    "        is_better = lambda current, best: current > best\n",
    "    elif PERFORMANCE_METRIC == 'mae':\n",
    "        best_score = np.inf\n",
    "        is_better = lambda current, best: current < best\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported performance metric. Please choose 'r2' or 'mae'.\")\n",
    "\n",
    "    performance_history = []\n",
    "    kf = KFold(n_splits=KFOLD_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    train_size_per_fold = X_shap_coarse.shape[0] * (KFOLD_SPLITS - 1) // KFOLD_SPLITS\n",
    "    min_samples_for_rf = max(RF_MIN_SAMPLES_LEAF, RF_MIN_SAMPLES_SPLIT)\n",
    "    if train_size_per_fold < min_samples_for_rf:\n",
    "        print(f\"Warning: Training samples per fold ({train_size_per_fold}) is less than the minimum required by Random Forest ({min_samples_for_rf}). Skipping iterative selection.\")\n",
    "        return list(X_shap_coarse.columns), []\n",
    "\n",
    "    print(\"Calculating baseline performance for the initial feature set...\")\n",
    "    initial_fold_scores = []\n",
    "    for train_idx, val_idx in tqdm(kf.split(X_shap_coarse), total=KFOLD_SPLITS, desc=\"Baseline Performance\"):\n",
    "        X_fold_train, y_fold_train = X_shap_coarse.iloc[train_idx][current_features], y.iloc[train_idx]\n",
    "        X_fold_val, y_fold_val = X_shap_coarse.iloc[val_idx][current_features], y.iloc[val_idx]\n",
    "        scaler_fold = StandardScaler().fit(X_fold_train)\n",
    "        X_fold_train_scaled = scaler_fold.transform(X_fold_train)\n",
    "        X_fold_val_scaled = scaler_fold.transform(X_fold_val)\n",
    "        rf_model = RandomForestRegressor(n_estimators=RF_N_ESTIMATORS, max_features=RF_MAX_FEATURES, min_samples_leaf=RF_MIN_SAMPLES_LEAF, min_samples_split=RF_MIN_SAMPLES_SPLIT, random_state=RANDOM_STATE, n_jobs=RF_N_JOBS)\n",
    "        try:\n",
    "            rf_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "            y_pred = rf_model.predict(X_fold_val_scaled)\n",
    "            if PERFORMANCE_METRIC == 'r2':\n",
    "                initial_fold_scores.append(r2_score(y_fold_val, y_pred))\n",
    "            elif PERFORMANCE_METRIC == 'mae':\n",
    "                initial_fold_scores.append(mean_absolute_error(y_fold_val, y_pred))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Model training or prediction failed during baseline calculation: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not initial_fold_scores:\n",
    "        print(\"Warning: Failed to produce any valid scores for the initial feature set. Skipping iterative selection.\")\n",
    "        return list(X_shap_coarse.columns), []\n",
    "\n",
    "    best_score = np.mean(initial_fold_scores)\n",
    "    performance_history.append({'num_features': len(current_features), PERFORMANCE_METRIC: best_score})\n",
    "    print(f\"  Initial feature set ({len(current_features)} features), Average {PERFORMANCE_METRIC.upper()}: {best_score:.4f}\")\n",
    "\n",
    "    print(\"Starting iterative feature removal...\")\n",
    "    \n",
    "    non_improvement_streak = 0\n",
    "    \n",
    "    while len(current_features) > 1:\n",
    "        current_iteration_shap_values_for_drop = []\n",
    "        for train_idx, val_idx in kf.split(X_shap_coarse):\n",
    "            X_fold_train, y_fold_train = X_shap_coarse.iloc[train_idx][current_features], y.iloc[train_idx]\n",
    "            scaler_fold = StandardScaler().fit(X_fold_train)\n",
    "            X_fold_train_scaled = scaler_fold.transform(X_fold_train)\n",
    "            rf_model = RandomForestRegressor(n_estimators=RF_N_ESTIMATORS, max_features=RF_MAX_FEATURES, min_samples_leaf=RF_MIN_SAMPLES_LEAF, min_samples_split=RF_MIN_SAMPLES_SPLIT, random_state=RANDOM_STATE, n_jobs=RF_N_JOBS)\n",
    "            try:\n",
    "                rf_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "                explainer = shap.TreeExplainer(rf_model)\n",
    "                shap_values_fold = explainer.shap_values(X_fold_train_scaled)\n",
    "                if shap_values_fold.ndim == 1:\n",
    "                    current_iteration_shap_values_for_drop.append(np.abs(shap_values_fold))\n",
    "                else:\n",
    "                    current_iteration_shap_values_for_drop.append(np.abs(shap_values_fold).mean(axis=0))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not current_iteration_shap_values_for_drop:\n",
    "            print(f\"  Warning: Iteration failed, could not calculate SHAP values. Stopping.\")\n",
    "            break\n",
    "\n",
    "        avg_iteration_shap = np.mean(current_iteration_shap_values_for_drop, axis=0)\n",
    "        feature_importances_current = pd.Series(avg_iteration_shap, index=current_features).sort_values(ascending=True)\n",
    "        feature_to_drop = feature_importances_current.index[0]\n",
    "        current_features.remove(feature_to_drop)\n",
    "        print(f\"  Attempting to remove least important feature: {feature_to_drop}\")\n",
    "\n",
    "        fold_scores_after_drop = []\n",
    "        for train_idx, val_idx in tqdm(kf.split(X_shap_coarse), total=KFOLD_SPLITS, desc=f\"Evaluating performance\"):\n",
    "            X_fold_train, y_fold_train = X_shap_coarse.iloc[train_idx][current_features], y.iloc[train_idx]\n",
    "            X_fold_val, y_fold_val = X_shap_coarse.iloc[val_idx][current_features], y.iloc[val_idx]\n",
    "            scaler_fold = StandardScaler().fit(X_fold_train)\n",
    "            X_fold_train_scaled = scaler_fold.transform(X_fold_train)\n",
    "            X_fold_val_scaled = scaler_fold.transform(X_fold_val)\n",
    "            rf_model = RandomForestRegressor(n_estimators=RF_N_ESTIMATORS, max_features=RF_MAX_FEATURES, min_samples_leaf=RF_MIN_SAMPLES_LEAF, min_samples_split=RF_MIN_SAMPLES_SPLIT, random_state=RANDOM_STATE, n_jobs=RF_N_JOBS)\n",
    "            try:\n",
    "                rf_model.fit(X_fold_train_scaled, y_fold_train)\n",
    "                y_pred = rf_model.predict(X_fold_val_scaled)\n",
    "                if PERFORMANCE_METRIC == 'r2':\n",
    "                    fold_scores_after_drop.append(r2_score(y_fold_val, y_pred))\n",
    "                elif PERFORMANCE_METRIC == 'mae':\n",
    "                    fold_scores_after_drop.append(mean_absolute_error(y_fold_val, y_pred))\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not fold_scores_after_drop:\n",
    "            print(f\"  Warning: Could not calculate valid scores after removing feature. Stopping.\")\n",
    "            break\n",
    "\n",
    "        current_avg_score = np.mean(fold_scores_after_drop)\n",
    "        performance_history.append({'num_features': len(current_features), PERFORMANCE_METRIC: current_avg_score})\n",
    "        print(f\"  Features: {len(current_features)}, Average {PERFORMANCE_METRIC.upper()}: {current_avg_score:.4f}\")\n",
    "\n",
    "        if is_better(current_avg_score, best_score):\n",
    "            best_score, best_features = current_avg_score, list(current_features)\n",
    "            print(f\"    -> Performance improved. Resetting patience counter. Best set size: {len(best_features)}\")\n",
    "            non_improvement_streak = 0\n",
    "        else:\n",
    "            non_improvement_streak += 1\n",
    "            print(f\"    -> No improvement (Streak: {non_improvement_streak}/{EARLY_STOPPING_PATIENCE})\")\n",
    "            if non_improvement_streak >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"    -> Early stopping triggered after {EARLY_STOPPING_PATIENCE} iterations with no improvement.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nIterative selection complete. Best feature set ({len(best_features)} features): {best_features}\")\n",
    "    return best_features, performance_history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"'{INPUT_FILE}' not found. Creating a dummy dataset for demonstration...\")\n",
    "        num_samples, feature_cols = 70, 162\n",
    "        df_list = [pd.DataFrame([f'Sample_{i+1}' for i in range(num_samples)], columns=['SampleID'])]\n",
    "        all_feature_names = [f'F{i+1}' for i in range(feature_cols)]\n",
    "        feature_data = np.random.rand(num_samples, len(all_feature_names))\n",
    "        df_features = pd.DataFrame(feature_data, columns=all_feature_names)\n",
    "        df_list.append(df_features)\n",
    "        df_dummy = pd.concat(df_list, axis=1)\n",
    "        y_values = 5 * df_dummy['F1'] + 3 * df_dummy['F2']**2 + np.random.randn(num_samples) * 0.5\n",
    "        df_dummy['Target'] = y_values\n",
    "        df_dummy.to_excel(INPUT_FILE, index=False)\n",
    "        print(f\"Dummy data saved to '{INPUT_FILE}'.\")\n",
    "\n",
    "    X_full, y_full, df_original = load_data_from_excel()\n",
    "    if not X_full.empty:\n",
    "        original_corr_matrix = X_full.corr(method='pearson')\n",
    "        original_corr_path = os.path.join(OUTPUT_DIR, f'original_feature_correlation_matrix_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        original_corr_matrix.to_excel(original_corr_path, index=True)\n",
    "        print(f\"Saved: Original feature correlation matrix -> {original_corr_path}\")\n",
    "\n",
    "    X_filtered, dropped_pearson_list = step1_filter_high_correlated_features(X_full, y_full)\n",
    "    if not X_filtered.empty and X_filtered.shape[1] > 1:\n",
    "        filtered_corr_matrix = X_filtered.corr(method='pearson')\n",
    "        filtered_corr_path = os.path.join(OUTPUT_DIR, f'stage1_filtered_correlation_matrix_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        filtered_corr_matrix.to_excel(filtered_corr_path, index=True)\n",
    "        print(f\"Saved: Stage 1 filtered correlation matrix -> {filtered_corr_path}\")\n",
    "\n",
    "    X_shap_coarse, dropped_shap = step2_embed_shap_coarse_selection(X_filtered, y_full)\n",
    "    final_features, performance_history = step3_wrapper_shap_iterative_selection(X_shap_coarse, y_full)\n",
    "\n",
    "    # Sort the final features according to their original column order\n",
    "    if final_features:\n",
    "        print(\"\\n--- Reordering final features based on original column order ---\")\n",
    "        original_feature_order = X_full.columns\n",
    "        sorted_final_features = [col for col in original_feature_order if col in final_features]\n",
    "        final_features = sorted_final_features\n",
    "        print(f\"Successfully reordered the final features.\")\n",
    "\n",
    "    print(f\"\\n--- Exporting final results to the '{OUTPUT_DIR}' directory ---\")\n",
    "    num_dropped_pearson = len(dropped_pearson_list)\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Stage': ['Initial Features', 'Stage 1: Pearson Correlation Filter', 'Stage 2: SHAP Coarse Selection', 'Stage 3: SHAP Iterative Selection'],\n",
    "        'Number of Features': [X_full.shape[1], X_filtered.shape[1], X_shap_coarse.shape[1], len(final_features)],\n",
    "        'Features Removed': [0, num_dropped_pearson, len(dropped_shap), X_shap_coarse.shape[1] - len(final_features)]\n",
    "    })\n",
    "    summary_path = os.path.join(OUTPUT_DIR, f'feature_engineering_summary_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "    summary_df.to_excel(summary_path, index=False)\n",
    "    print(f\"Saved: Feature engineering summary -> {summary_path}\")\n",
    "\n",
    "    history_path = os.path.join(OUTPUT_DIR, f'performance_iteration_history_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "    pd.DataFrame(performance_history).to_excel(history_path, index=False)\n",
    "    print(f\"Saved: Performance iteration history -> {history_path}\")\n",
    "\n",
    "    if len(final_features) > 1:\n",
    "        final_features_df = df_original[final_features] # Use the sorted version\n",
    "        final_corr_matrix = final_features_df.corr(method='pearson')\n",
    "        final_corr_path = os.path.join(OUTPUT_DIR, f'final_features_correlation_matrix_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        final_corr_matrix.to_excel(final_corr_path, index=True)\n",
    "        print(f\"Saved: Final features correlation matrix -> {final_corr_path}\")\n",
    "\n",
    "    if final_features:\n",
    "        cols_to_export = []\n",
    "        # Assume the first column is a sample ID\n",
    "        if not df_original.empty and df_original.columns[0] not in final_features and df_original.columns[0] != y_full.name:\n",
    "            cols_to_export.append(df_original.columns[0])\n",
    "        \n",
    "        cols_to_export.extend(final_features) # Use the sorted version\n",
    "        if y_full.name in df_original.columns:\n",
    "            cols_to_export.append(y_full.name)\n",
    "        \n",
    "        # Remove duplicates in case the ID column was also selected as a feature\n",
    "        unique_cols_to_export = list(dict.fromkeys(cols_to_export))\n",
    "\n",
    "        dataset_path = os.path.join(OUTPUT_DIR, f'final_selected_dataset_{time.strftime(\"%Y%m%d_%H%M%S\")}.xlsx')\n",
    "        df_original[unique_cols_to_export].to_excel(dataset_path, index=False)\n",
    "        print(f\"Saved: Final selected dataset -> {dataset_path}\")\n",
    "    else:\n",
    "        print(\"No features were selected in the final set. The final dataset was not saved.\")\n",
    "\n",
    "    print(\"\\nAll processing is complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (matsci-ai)",
   "language": "python",
   "name": "matsci-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
